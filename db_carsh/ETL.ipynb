{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f777c6-a3b7-45d1-ba21-00b97a342395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f47df-d207-4d74-a4c4-07bc7b6eaf9f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41c3b1-1df0-475c-bcd8-5232ecefe8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load fatal crashes data\n",
    "# Load data from a specific sheet named \"BITRE_Fatal_Crash\",skipping non-data rows\n",
    "crash_df = pd.read_excel(\n",
    "    'bitre_fatal_crashes_dec2024.xlsx', \n",
    "    sheet_name='BITRE_Fatal_Crash',\n",
    "    skiprows=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6fcb3",
   "metadata": {},
   "source": [
    "## Check Na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to count missing values in a Series\n",
    "def count_missing(series):\n",
    "    # Treat actual NaN as missing\n",
    "    mask = series.isna()\n",
    "    # Treat blank strings, 'Unknown', 'Undetermined' as missing\n",
    "    mask |= series.astype(str).str.strip().isin(['', 'Unknown', 'Undetermined'])\n",
    "    # Treat numeric -9 as missing\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        mask |= (series == -9)\n",
    "    return mask.sum()\n",
    "\n",
    "# Apply to every column\n",
    "missing_counts = crash_df.apply(count_missing)\n",
    "\n",
    "# Print counts per column, also check col name\n",
    "print(missing_counts.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6113a",
   "metadata": {},
   "source": [
    "## Load fatalities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d305d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fatalities_df = pd.read_excel(\n",
    "    'bitre_fatalities_dec2024.xlsx', \n",
    "    sheet_name='BITRE_Fatality',\n",
    "    skiprows=4\n",
    ")\n",
    "\n",
    "missing_counts_fatalities = fatalities_df.apply(count_missing)\n",
    "\n",
    "# Print all na counts per column\n",
    "print(missing_counts_fatalities.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85c759-2c1d-4e08-a2c6-5487ee4fd257",
   "metadata": {},
   "source": [
    "## Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244014b3-6161-47c6-83a2-49eff77de168",
   "metadata": {},
   "source": [
    "### 1.Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdd7c4-e7ac-4ff1-9622-3fd1844f71fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24 unique time combinations.\n",
      "   Time_ID  Time Time of Day    Time Period\n",
      "0        0     0       Night       Midnight\n",
      "1        1     1       Night       Midnight\n",
      "2        2     2       Night       Midnight\n",
      "3        3     3       Night       Midnight\n",
      "4        4     4       Night  Early Morning\n"
     ]
    }
   ],
   "source": [
    "# update only keep time data now, keep month, year and weekday in fact table\n",
    "# there are 39 blank for time, 40 unknown for time_of_day. and they have no location data -> remove directly from raw dataset, same for fatalities dataset(43 rows)\n",
    "\n",
    "crash_df['Time'] = crash_df['Time'].astype(str).str.strip()\n",
    "crash_df['Time'] = pd.to_datetime(crash_df['Time'], format='%H:%M:%S', errors='coerce').dt.hour\n",
    "crash_df['Time'] = crash_df['Time'].astype(int)\n",
    "\n",
    "# 2. Select 'Time', drop duplicatesï¼Œsort\n",
    "dim_time_df = crash_df[['Time']].drop_duplicates().sort_values(by='Time', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Add 'Time_ID' column (starting from 1)\n",
    "dim_time_df.insert(0, 'Time_ID', range(0, len(dim_time_df)))\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    if 6 <= hour <= 18:\n",
    "        return 'Day'\n",
    "    elif 0 <= hour <= 5 or 19 <= hour <= 23:\n",
    "        return 'Night'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "dim_time_df['Time of Day'] = dim_time_df['Time'].apply(get_time_of_day)\n",
    "# 5. Define time period based on hour\n",
    "def get_time_period(hour):\n",
    "    if 0 <= hour <= 3:\n",
    "        return 'Midnight'\n",
    "    elif 4 <= hour <= 7:\n",
    "        return 'Early Morning'\n",
    "    elif 8 <= hour <= 11:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour <= 15:\n",
    "        return 'Afternoon'\n",
    "    elif 16 <= hour <= 19:\n",
    "        return 'Evening'\n",
    "    elif 20 <= hour <= 23:\n",
    "        return 'Late Night'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "dim_time_df['Time Period'] = dim_time_df['Time'].apply(get_time_period)\n",
    "\n",
    "# Export to CSV without the DataFrame index\n",
    "dim_time_df.to_csv('dim_time.csv', index=False)\n",
    "\n",
    "print(f\" {len(dim_time_df)} unique time combinations.\")\n",
    "print(dim_time_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46277ae-70ad-42a6-80cb-b8bd92b26f41",
   "metadata": {},
   "source": [
    "### 2.State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b3d5b-b8d6-4c11-8b86-f33bee4c4fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8 unique time combinations.\n",
      "   State_ID State\n",
      "0         1   NSW\n",
      "1         2   Tas\n",
      "2         3   Vic\n",
      "3         4   Qld\n",
      "4         5    SA\n"
     ]
    }
   ],
   "source": [
    "# instead put all location details in one dimension, i separated into state and remoteness, keep iga in fact table and ignore sa4\n",
    "dim_state_df = crash_df[['State']].copy().drop_duplicates()\n",
    "\n",
    "# 3. Reset index after dropping duplicates\n",
    "dim_state_df = dim_state_df.reset_index(drop=True)\n",
    "\n",
    "# 4. Add 'Time_ID' column (starting from 1)\n",
    "dim_state_df.insert(0, 'State_ID', range(1, len(dim_state_df) + 1))\n",
    "\n",
    "# 5. Export to CSV without the DataFrame index\n",
    "dim_state_df.to_csv('dim_state.csv', index=False)\n",
    "\n",
    "print(f\" {len(dim_state_df)} unique time combinations.\")\n",
    "print(dim_state_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d8971-ba1b-40a5-8fbe-e16f239e1847",
   "metadata": {},
   "source": [
    "### 3.Vehicle involvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d458202-d446-4499-becc-c85dc02c17e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_9504/745254592.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  vehicle_df.replace({'Yes': 1, 'No': 0}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicle_ID</th>\n",
       "      <th>Bus_Involved</th>\n",
       "      <th>Heavy_Truck</th>\n",
       "      <th>Articulated_Truck</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bus not involved; Heavy rigid truck not involv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>Bus involvement unknown; Heavy rigid truck inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bus not involved; Heavy rigid truck not involv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bus involved; Heavy rigid truck not involved; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Bus not involved; Heavy rigid truck involved; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vehicle_ID  Bus_Involved  Heavy_Truck  Articulated_Truck  \\\n",
       "0           1             0            0                  0   \n",
       "1           2            -9           -9                 -9   \n",
       "2           3             0            0                  1   \n",
       "3           4             1            0                  1   \n",
       "4           5             0            1                  0   \n",
       "\n",
       "                                         Description  \n",
       "0  Bus not involved; Heavy rigid truck not involv...  \n",
       "1  Bus involvement unknown; Heavy rigid truck inv...  \n",
       "2  Bus not involved; Heavy rigid truck not involv...  \n",
       "3  Bus involved; Heavy rigid truck not involved; ...  \n",
       "4  Bus not involved; Heavy rigid truck involved; ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract relevant fields related to vehicle involvement\n",
    "vehicle_df = crash_df[['Bus \\nInvolvement', 'Heavy Rigid Truck Involvement', 'Articulated Truck Involvement']].copy()\n",
    "\n",
    "# Rename columns to standardized names\n",
    "vehicle_df.columns = ['Bus_Involved', 'Heavy_Truck', 'Articulated_Truck']\n",
    "\n",
    "# yes 1, no 0, -9 ua\n",
    "vehicle_df.replace({'Yes': 1, 'No': 0}, inplace=True)\n",
    "vehicle_df = vehicle_df.infer_objects(copy=False)\n",
    "\n",
    "# Create description column based on combinations\n",
    "def describe_vehicle(row):\n",
    "    desc_parts = []\n",
    "\n",
    "    for label, value in {\n",
    "        'Bus': row['Bus_Involved'],\n",
    "        'Heavy rigid truck': row['Heavy_Truck'],\n",
    "        'Articulated truck': row['Articulated_Truck']\n",
    "    }.items():\n",
    "        if value == 1:\n",
    "            desc_parts.append(f\"{label} involved\")\n",
    "        elif value == 0:\n",
    "            desc_parts.append(f\"{label} not involved\")\n",
    "        elif value == -9:\n",
    "            desc_parts.append(f\"{label} involvement unknown\")\n",
    "        else:\n",
    "            desc_parts.append(f\"{label} value invalid\")\n",
    "\n",
    "    return \"; \".join(desc_parts)\n",
    "\n",
    "\n",
    "vehicle_df['Description'] = vehicle_df.apply(describe_vehicle, axis=1)\n",
    "# Remove duplicate records to build a unique dimension table\n",
    "dim_vehicle_df = vehicle_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Add a surrogate primary key\n",
    "dim_vehicle_df.insert(0, 'Vehicle_ID', range(1, len(dim_vehicle_df) + 1))\n",
    "\n",
    "# Optional: save the dimension table to CSV\n",
    "dim_vehicle_df.to_csv('dim_vehicle.csv', index=False)\n",
    "\n",
    "# Display the resulting vehicle dimension table\n",
    "dim_vehicle_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f35f9-6783-4587-856d-5cb5319398f4",
   "metadata": {},
   "source": [
    "### 4.Road Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db721945-65eb-4d7e-b2b6-cc76947674a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Road_ID</th>\n",
       "      <th>Road_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Arterial Road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Local Road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>National or State Highway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Undetermined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Sub-arterial Road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Road_ID                  Road_Type\n",
       "0        1              Arterial Road\n",
       "1        2                 Local Road\n",
       "2        3  National or State Highway\n",
       "3        4               Undetermined\n",
       "4        5          Sub-arterial Road"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract the relevant column for road type\n",
    "road_df = crash_df[['National Road Type']].copy()\n",
    "\n",
    "# Rename the column for consistency\n",
    "road_df.columns = ['Road_Type']\n",
    "\n",
    "# Remove rows with missing values and drop duplicates\n",
    "road_df = road_df.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Add a surrogate primary key\n",
    "road_df.insert(0, 'Road_ID', range(1, len(road_df) + 1))\n",
    "\n",
    "# Optional: save the dimension table to CSV\n",
    "road_df.to_csv('dim_road_type.csv', index=False)\n",
    "\n",
    "# Display the resulting road type dimension table\n",
    "road_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48923411-45d2-45ee-a2a7-d9103e8fc1fa",
   "metadata": {},
   "source": [
    "### 5.Holiday flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a997fd8-7f15-4b17-9f3b-b716314b7e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Holiday_ID</th>\n",
       "      <th>Christmas_Flag</th>\n",
       "      <th>Easter_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Holiday_ID Christmas_Flag Easter_Flag\n",
       "0           1            Yes          No\n",
       "1           2             No          No\n",
       "2           3             No         Yes"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract holiday-related columns\n",
    "holiday_df = crash_df[['Christmas Period', 'Easter Period']].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "holiday_df.columns = ['Christmas_Flag', 'Easter_Flag']\n",
    "\n",
    "# Remove rows with missing values and drop duplicate combinations\n",
    "holiday_df = holiday_df.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Add a surrogate primary key\n",
    "holiday_df.insert(0, 'Holiday_ID', range(1, len(holiday_df) + 1))\n",
    "\n",
    "# Optional: save to CSV\n",
    "holiday_df.to_csv('dim_holiday.csv', index=False)\n",
    "\n",
    "# Display the result\n",
    "holiday_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09800a51-d4e8-444f-9c90-869165b4f321",
   "metadata": {},
   "source": [
    "### 6.Speed Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b796ecf5-f81c-4728-8412-6f8d169a8ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_1012/1915951888.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  speed_df['Speed_Value'] = speed_df['Speed_Value'].replace({'<40': 35, -9: 0})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speed_ID</th>\n",
       "      <th>Speed_Value</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5 km/h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10 km/h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15 km/h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>20 km/h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Speed_ID  Speed_Value Description\n",
       "0         1            0     Unknown\n",
       "1         2            5      5 km/h\n",
       "2         3           10     10 km/h\n",
       "3         4           15     15 km/h\n",
       "4         5           20     20 km/h"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the speed limit column\n",
    "speed_df = crash_df[['Speed Limit']].copy()\n",
    "\n",
    "# Rename column for consistency\n",
    "speed_df.columns = ['Speed_Value']\n",
    "\n",
    "# Replace special values\n",
    "# '<40' is a string, replace it with 35\n",
    "# -9 is used for unknown values, replace with 0\n",
    "speed_df['Speed_Value'] = speed_df['Speed_Value'].replace({'<40': 35, -9: 0})\n",
    "\n",
    "# Convert to integer\n",
    "speed_df['Speed_Value'] = speed_df['Speed_Value'].astype(int)\n",
    "\n",
    "# Drop duplicates\n",
    "speed_df = speed_df.drop_duplicates().reset_index(drop=True)\n",
    "speed_df = speed_df.sort_values(by='Speed_Value').reset_index(drop=True)\n",
    "\n",
    "# Add surrogate key\n",
    "speed_df.insert(0, 'Speed_ID', range(1, len(speed_df) + 1))\n",
    "\n",
    "# Add Description based on speed limit values\n",
    "speed_df['Description'] = speed_df['Speed_Value'].apply(\n",
    "    lambda x: 'Unknown' if x == 0 else ('<40 km/h' if x == 35 else f'{x} km/h')\n",
    "\n",
    ")\n",
    "\n",
    "# Optional: save to CSV\n",
    "speed_df.to_csv('dim_speed_limit.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "speed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed12017a-3195-462d-9688-c82c780c3973",
   "metadata": {},
   "source": [
    "### 7.Crash Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9efb7a7a-b528-4d36-b5e6-e61f6c8c0c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crash_Type_ID</th>\n",
       "      <th>Crash_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Multiple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Crash_Type_ID Crash_Type\n",
       "0              1     Single\n",
       "1              2   Multiple"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract the crash type column\n",
    "crash_type_df = crash_df[['Crash Type']].copy()\n",
    "\n",
    "# Rename column for consistency\n",
    "crash_type_df.columns = ['Crash_Type']\n",
    "\n",
    "crash_type_df = crash_type_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Add a surrogate primary key\n",
    "crash_type_df.insert(0, 'Crash_Type_ID', range(1, len(crash_type_df) + 1))\n",
    "\n",
    "# Optional: save to CSV\n",
    "crash_type_df.to_csv('dim_crash_type.csv', index=False)\n",
    "\n",
    "# Display the result\n",
    "crash_type_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1aa87",
   "metadata": {},
   "source": [
    "### 8.National Remoteness Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eef79257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Remoteness_ID           Remoteness_Areas\n",
      "0              0                    Unknown\n",
      "1              1   Inner Regional Australia\n",
      "2              2   Outer Regional Australia\n",
      "3              3  Major Cities of Australia\n",
      "4              4      Very Remote Australia\n",
      "5              5           Remote Australia\n"
     ]
    }
   ],
   "source": [
    "remoteness_df = crash_df[['National Remoteness Areas']].copy()\n",
    "\n",
    "# Rename column for consistency\n",
    "remoteness_df.columns = ['Remoteness_Areas']\n",
    "remoteness_df['Remoteness_Areas'] = remoteness_df['Remoteness_Areas'].astype(str).str.strip()\n",
    "\n",
    "# Mark blank or 'Unknown' as 'Unknown'\n",
    "remoteness_df['Remoteness_Areas'] = remoteness_df['Remoteness_Areas'].replace(['', 'Unknown'], 'Unknown')\n",
    "\n",
    "# Separate unknown and known values\n",
    "unknown_df = pd.DataFrame([['Unknown']], columns=['Remoteness_Areas'])\n",
    "known_df = remoteness_df[remoteness_df['Remoteness_Areas'] != 'Unknown'].drop_duplicates()\n",
    "\n",
    "# Combine with 'Unknown' on top\n",
    "final_df = pd.concat([unknown_df, known_df.drop_duplicates()]).reset_index(drop=True)\n",
    "\n",
    "final_df.insert(0, 'Remoteness_ID', range(len(final_df)))\n",
    "\n",
    "final_df.to_csv('dim_remoteness.csv', index=False)\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13beea35-37aa-452c-b759-8f2921b053ea",
   "metadata": {},
   "source": [
    "## Fact Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c010a-b73f-467e-859b-c1941395b34d",
   "metadata": {},
   "source": [
    "### Fact Table 1ï¼šFact_Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4521d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Crash_ID  Time_ID  Speed_ID  State_ID  Remoteness_ID  Road_type_ID  \\\n",
      "0  20241115        4         1         1              1             1   \n",
      "1  20241125        6         2         1              1             2   \n",
      "2  20246013        9         3         2              1             2   \n",
      "3  20241002       10         1         1              2             3   \n",
      "4  20242261       11         4         3              0             4   \n",
      "5  20243185       13         1         4              1             3   \n",
      "6  20244016       13         1         5              2             5   \n",
      "7  20245001       17         5         6              0             4   \n",
      "8  20243168       19         6         4              1             2   \n",
      "9  20246003       19         3         2              1             2   \n",
      "\n",
      "   Crash_type_ID  Vehicle_ID  Holiday_ID  Month  Year Dayweek  \\\n",
      "0              1           1           1     12  2024  Friday   \n",
      "1              1           1           2     12  2024  Friday   \n",
      "2              2           1           1     12  2024  Friday   \n",
      "3              2           1           2     12  2024  Friday   \n",
      "4              2           2           2     12  2024  Friday   \n",
      "5              2           1           2     12  2024  Friday   \n",
      "6              1           1           1     12  2024  Friday   \n",
      "7              1           1           2     12  2024  Friday   \n",
      "8              2           1           2     12  2024  Friday   \n",
      "9              2           1           2     12  2024  Friday   \n",
      "\n",
      "            LGA_name  Number Fatalities  \n",
      "0        Wagga Wagga                  1  \n",
      "1         Hawkesbury                  1  \n",
      "2  Northern Midlands                  1  \n",
      "3  Armidale Regional                  1  \n",
      "4            Unknown                  1  \n",
      "5     Lockyer Valley                  1  \n",
      "6          Wakefield                  1  \n",
      "7            Unknown                  1  \n",
      "8     Southern Downs                  1  \n",
      "9           Brighton                  1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_1012/3373127826.py:102: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  fact_crash_data['Bus_Involved'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_1012/3373127826.py:102: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fact_crash_data['Bus_Involved'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_1012/3373127826.py:103: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  fact_crash_data['Heavy_Truck'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_1012/3373127826.py:103: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fact_crash_data['Heavy_Truck'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_1012/3373127826.py:104: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  fact_crash_data['Articulated_Truck'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_1012/3373127826.py:104: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fact_crash_data['Articulated_Truck'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_1012/3373127826.py:111: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fact_crash_data['Speed_Value'] = fact_crash_data['Speed_Value'].replace({'<40': 35, -9: 0}).astype(int)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crash_df = pd.read_excel('bitre_fatal_crashes_dec2024.xlsx', sheet_name='BITRE_Fatal_Crash', skiprows=4)\n",
    "\n",
    "# select columns and rename\n",
    "fact_crash_data = crash_df.rename(columns={\n",
    "    'Crash ID': 'Crash_ID',\n",
    "    'State': 'State',\n",
    "    'National Remoteness Areas': 'Remoteness_area',\n",
    "    'National Road Type': 'Road_Type',\n",
    "    'National LGA Name 2021': 'LGA_name',\n",
    "    'Crash Type': 'Crash_Type',\n",
    "    'Speed Limit': 'Speed_Value',\n",
    "    'Christmas Period': 'Christmas_Flag',\n",
    "    'Easter Period': 'Easter_Flag',\n",
    "    'Bus \\nInvolvement': 'Bus_Involved',\n",
    "    'Heavy Rigid Truck Involvement': 'Heavy_Truck',\n",
    "    'Articulated Truck Involvement': 'Articulated_Truck'\n",
    "})[\n",
    "    ['Crash_ID', 'State', 'Remoteness_area', 'Road_Type', 'Month', 'Year', 'Dayweek', 'LGA_name', 'Number Fatalities', 'Crash_Type','Speed_Value','Christmas_Flag', 'Easter_Flag', 'Bus_Involved', 'Heavy_Truck', 'Articulated_Truck']\n",
    "].copy()\n",
    "\n",
    "# Fill blanks with Unknown\n",
    "fact_crash_data.fillna({\n",
    "    'LGA_name': 'Unknown',\n",
    "    'Remoteness_area': 'Unknown'\n",
    "}, inplace=True)\n",
    "\n",
    "# identify mapping for State and its id\n",
    "state_mapping = {\n",
    "    'NSW': 1,\n",
    "    'Tas': 2,\n",
    "    'Vic': 3,\n",
    "    'Qld': 4,\n",
    "    'SA': 5,\n",
    "    'WA': 6,\n",
    "    'ACT': 7,\n",
    "    'NT': 8\n",
    "}\n",
    "remoteness_mapping = {\n",
    "    'Unknown': 0,\n",
    "    'Inner Regional Australia': 1,\n",
    "    'Outer Regional Australia': 2,\n",
    "    'Major Cities of Australia': 3,\n",
    "    'Very Remote Australia': 4,\n",
    "    'Remote Australia': 5\n",
    "}\n",
    "\n",
    "road_type_mapping = {\n",
    "    'Arterial Road': 1,\n",
    "    'Local Road': 2,\n",
    "    'National or State Highway': 3,\n",
    "    'Undetermined': 4,\n",
    "    'Sub-arterial Road': 5,\n",
    "    'Collector Road': 6,\n",
    "    'Pedestrian Thoroughfare': 7,\n",
    "    'Access road': 8,\n",
    "    'Busway': 9\n",
    "}\n",
    "\n",
    "crash_type_mapping = {\n",
    "    'Single': 1, 'Multiple': 2, 'Unknown': 0\n",
    "}\n",
    "speed_mapping = {\n",
    "    0: 4, 5: 12, 10: 11, 15: 17, 20: 10, 25: 16, 30: 14, 35: 15, 40: 9, 50: 3,\n",
    "    60: 6, 70: 7, 75: 18, 80: 2, 90: 5, 100: 1, 110: 8, 130: 13\n",
    "}\n",
    "\n",
    "holiday_map = {\n",
    "    ('Yes', 'No'): 1,\n",
    "    ('No', 'No'): 2,\n",
    "    ('No', 'Yes'): 3\n",
    "}\n",
    "\n",
    "# Vehicle involvement mapping\n",
    "vehicle_map = {\n",
    "    (0, 0, 0): 1,\n",
    "    (-9, -9, -9): 2,\n",
    "    (0, 0, 1): 3,\n",
    "    (1, 0, 1): 4,\n",
    "    (0, 1, 0): 5,\n",
    "    (-9, 1, -9): 6,\n",
    "    (0, 1, 1): 7,\n",
    "    (1, 0, 0): 8,\n",
    "    (1, 1, 0): 9,\n",
    "    (-9, -9, 1): 10,\n",
    "    (0, -9, 0): 11,\n",
    "    (0, -9, 1): 12,\n",
    "    (1, -9, 0): 13,\n",
    "    (1, -9, 1): 14\n",
    "}\n",
    "\n",
    "\n",
    "# Extract hour from Time column as Time_ID after crash_id\n",
    "crash_df['Time'] = crash_df['Time'].astype(str).str.strip()\n",
    "crash_df['Time'] = pd.to_datetime(crash_df['Time'], format='%H:%M:%S', errors='coerce').dt.hour\n",
    "crash_df['Time'] = crash_df['Time'].fillna(0).astype(int)\n",
    "fact_crash_data.insert(1, 'Time_ID', crash_df['Time'])\n",
    "\n",
    "\n",
    "# Replace 'Yes', 'No', and '-9' with respective values\n",
    "fact_crash_data['Bus_Involved'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
    "fact_crash_data['Heavy_Truck'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
    "fact_crash_data['Articulated_Truck'].replace({'Yes': 1, 'No': 0, '-9': -9}, inplace=True)\n",
    "\n",
    "# Create Vehicle_ID based on Bus_Involved, Heavy_Truck, and Articulated_Truck columns\n",
    "fact_crash_data['Vehicle_ID'] = list(zip(fact_crash_data['Bus_Involved'], fact_crash_data['Heavy_Truck'], fact_crash_data['Articulated_Truck']))\n",
    "fact_crash_data['Vehicle_ID'] = fact_crash_data['Vehicle_ID'].map(vehicle_map)\n",
    "\n",
    "# Speed valueï¼š<40 -> 35ï¼Œ-9 -> 0\n",
    "fact_crash_data['Speed_Value'] = fact_crash_data['Speed_Value'].replace({'<40': 35, -9: 0}).astype(int)\n",
    "fact_crash_data.insert(2, 'Speed_ID', fact_crash_data['Speed_Value'].map(speed_mapping).fillna(4).astype(int))\n",
    "\n",
    "# Map Holiday_ID\n",
    "fact_crash_data['Holiday_ID'] = list(zip(fact_crash_data['Christmas_Flag'], fact_crash_data['Easter_Flag']))\n",
    "fact_crash_data['Holiday_ID'] = fact_crash_data['Holiday_ID'].map(holiday_map)\n",
    "\n",
    "# add other ids \n",
    "fact_crash_data['State_ID'] = fact_crash_data['State'].map(state_mapping).fillna(0).astype(int)\n",
    "fact_crash_data['Remoteness_ID'] = fact_crash_data['Remoteness_area'].map(remoteness_mapping).astype(int)\n",
    "fact_crash_data['Road_type_ID'] = fact_crash_data['Road_Type'].map(road_type_mapping).astype(int)\n",
    "fact_crash_data['Crash_type_ID'] = fact_crash_data['Crash_Type'].map(crash_type_mapping).astype(int)\n",
    "\n",
    "\n",
    "fact_crash_data.drop(columns=['State', 'Remoteness_area','Road_Type', 'Crash_Type','Speed_Value', 'Christmas_Flag', 'Easter_Flag', 'Bus_Involved', 'Heavy_Truck', 'Articulated_Truck'], inplace=True)\n",
    "#reorder to match db schema\n",
    "cols = ['Crash_ID', 'Time_ID', 'Speed_ID','State_ID', 'Remoteness_ID', 'Road_type_ID', 'Crash_type_ID','Vehicle_ID','Holiday_ID',\n",
    "        'Month', 'Year', 'Dayweek', 'LGA_name', 'Number Fatalities']\n",
    "fact_crash_data = fact_crash_data[cols]\n",
    "fact_crash_data.to_csv('fact_crash_1.csv', index=False)\n",
    "\n",
    "print(fact_crash_data.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a54245-4759-4b48-a6e9-8ee6e1770ceb",
   "metadata": {},
   "source": [
    "### Fact Table 2ï¼šFact_Fatality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76fa4d32-fa6a-4932-b5c9-b25bab48d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fatality_id  Crash_ID  Road_User  Gender  Age Age_Group\n",
      "0            1  20241115     Driver    Male   74  65_to_74\n",
      "1            2  20241125     Driver  Female   19  17_to_25\n",
      "2            3  20246013     Driver  Female   33  26_to_39\n",
      "3            4  20241002     Driver  Female   32  26_to_39\n",
      "4            5  20242261  Passenger    Male   62  40_to_64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fatality_df = pd.read_excel('bitre_fatalities_dec2024.xlsx', sheet_name='BITRE_Fatality', skiprows=4)\n",
    "\n",
    "# Select and rename the relevant columns for fatalities data\n",
    "fatalities_data = fatality_df.rename(columns={\n",
    "    'Crash ID': 'Crash_ID',\n",
    "    'Road User': 'Road_User',\n",
    "    'Gender': 'Gender',\n",
    "    'Age': 'Age',\n",
    "    'Age Group': 'Age_Group'\n",
    "})[\n",
    "    ['Crash_ID', 'Road_User', 'Gender', 'Age', 'Age_Group']\n",
    "].copy()\n",
    "fatalities_data['Road_User'] = fatalities_data['Road_User'].str.strip()\n",
    "fatalities_data['Gender'] = fatalities_data['Gender'].str.strip()\n",
    "fatalities_data['Age_Group'] = fatalities_data['Age_Group'].str.strip()\n",
    "\n",
    "# Replace invalid values ('Other' and '-9') with 'Unknown' in 'Road_User', 'Gender', 'Age', and 'Age_Group' columns\n",
    "fatalities_data['Road_User'] = fatalities_data['Road_User'].replace({'Other/-9': 'Unknown'})\n",
    "fatalities_data['Gender'] = fatalities_data['Gender'].fillna('Unknown')\n",
    "\n",
    "fatalities_data['Age_Group'] = fatalities_data['Age_Group'].fillna('Unknown')\n",
    "\n",
    "\n",
    "# Create a unique fatality_id for each record (this will act as the primary key)\n",
    "fatalities_data['fatality_id'] = range(1, len(fatalities_data) + 1)\n",
    "\n",
    "# Rearrange columns to match the required schema\n",
    "fatalities_data = fatalities_data[['fatality_id', 'Crash_ID', 'Road_User', 'Gender', 'Age', 'Age_Group']]\n",
    "\n",
    "# Save the resulting DataFrame to a CSV file\n",
    "fatalities_data.to_csv('fact_fatalities.csv', index=False)\n",
    "\n",
    "# Print the first few rows to verify the result\n",
    "print(fatalities_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10d148f-03c9-4eb8-89ac-d4aa08a53e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Speed_Value  Speed_ID\n",
      "0          100        16\n",
      "1           80        14\n",
      "2           50        10\n",
      "3          100        16\n",
      "4            0         1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/97mgdx1j24nbpzp1_h96gl_40000gn/T/ipykernel_9322/2713569223.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .replace({'<40': 35, -9: 0})\n"
     ]
    }
   ],
   "source": [
    "# reload speed value\n",
    "import pandas as pd\n",
    "update_df = pd.read_excel('bitre_fatal_crashes_dec2024.xlsx', sheet_name='BITRE_Fatal_Crash', skiprows=4)\n",
    "# Speed valueï¼š<40 -> 35ï¼Œ-9 -> 0\n",
    "update_df['Speed_Value'] = (\n",
    "    update_df['Speed Limit']\n",
    "    .replace({'<40': 35, -9: 0})\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "speed_mapping = {\n",
    "     0:  1,\n",
    "     5:  2,\n",
    "    10:  3,\n",
    "    15:  4,\n",
    "    20:  5,\n",
    "    25:  6,\n",
    "    30:  7,\n",
    "    35:  8,\n",
    "    40:  9,\n",
    "    50: 10,\n",
    "    60: 11,\n",
    "    70: 12,\n",
    "    75: 13,\n",
    "    80: 14,\n",
    "    90: 15,\n",
    "   100: 16,\n",
    "   110: 17,\n",
    "   130: 18\n",
    "}\n",
    "\n",
    "speed_data = pd.DataFrame({\n",
    "    'Speed_Value': update_df['Speed_Value']\n",
    "})\n",
    "speed_data['Speed_ID'] = speed_data['Speed_Value'].map(speed_mapping)\n",
    "# only keep id\n",
    "speed_data[['Speed_ID']].to_csv('speed.csv', index=False)\n",
    "\n",
    "print(speed_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
